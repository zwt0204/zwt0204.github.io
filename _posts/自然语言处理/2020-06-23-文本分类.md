---
layout:     post
title:      "classfication"
subtitle:   " \"分类\""
date:       2020-07-01 15:30:00 
mathjax: true
author:     "zwt"
header-img: "img/post-bg-2015.jpg"
catalog: false
tags:
    - nlp
---
* TOC
{:toc}

# 1


# 2


# 一些trick

对于分词器：在有接预训练词向量的前提下，如果可以找到预训练词向量所使用分词器最好，这样可以防止带来oov问题。找不到也要尝试使用与预训练词向量所使用的分词器最接近的分词器。

对于中文字向量：最好也预训练一下，同时预训练时候的窗口开大一点。

对于数据噪声：一种是数据集杂乱，比如文本的口语或者是生成的文本，一种是标签标注错误。对于前一种，直接使用字级别的embedding不使用预训练，或者使用fasttext训练一份词向量，将窗口设置为1-2,（这样有助于捕获错别字，一般错别字就是同音异形的）小窗口的fasttext就可以学出类似似乎和是乎这种错别字。另一种是标签错误：分析bad cases来针对性的进行纠正或者删除。

对于dropout可以加在embedding、pooling、FC之后

对于二分类：不一定非要使用sigmod，也可以使用softmax

对于多标签分类：可以使用binary-cross-entropy做baseline，将多标签分类转化为多个二分类

对于类别不均衡：[参考](https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247484993&idx=1&sn=0bd32089a638e5a1b48239656febb6e0&chksm=970c2e97a07ba7818d63dddbb469486dccb369ecc11f38ffdea596452b9e5bf65772820a8ac9&token=407616831&lang=zh_CN#rd)

在做具体的任务之前先分析数据的分布，然后再看应该使用什么模型作为baseline，一般基线选择TextCNN。当然如果数据集的n-gram很强，则直接使用cnn，如果需要看一下才知道意思就选择lstm。