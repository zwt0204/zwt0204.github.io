---
layout:     post
title:      "pyspark"
subtitle:   " \"pyspark\""
date:       2021-08-03 18:00:00
mathjax: true
author:     "zwt"
header-img: "img/post-bg-2015.jpg"
catalog: false
tags:
    - spark
---
* TOC
{:toc}

# 操作
```
1.pandas转pyspark：spark_df = sqlContest.createDataFrame(df)
2.一行拆分多行：movies.withColumn("genre", explode(split($"genre", "[|]"))).show
3.df.groupby("id").agg(F.collect_set("code"))
4.一列拆分多列：val Df2 = Df1.withColumn("splitcol",split(col("contents"), ",")).select(
            col("splitcol").getItem(0).as("col1"),
            col("splitcol").getItem(1).as("col2"),
            col("splitcol").getItem(2).as("col3")
        ).drop("splitcol");
5. 列重命名：df.withColumnRenamed("user_id", "user")
6. service_result = df_info.withColumn("service_explode", explode(col("service_prod_code")))
7. 添加id列：df.withColumn("id", monotonically_increasing_id)
8. 交集：sentenceDataFrame1.select("sentence").intersect(sentenceDataFrame.select("sentence"))
9. union并集
10. subtract差集
11. 过滤：numeric_filtered = df.where((col('LOW')    != 'null'))).show()
12. 修改列名：.withColumnRenamed
13. 新增一列：df.withColumn('column', lit(10))
14. 连接mysql操作：df.write.format('jdbc').options(url='', driver='',dbtable='',user='',password='').model('append').save()
15. 去重：df = df.dropDuplicates()
```