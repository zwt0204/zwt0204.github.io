---
layout:     post
title:      "torch基础"
subtitle:   " \"torch\""
date:       2021-07-13 18:00:00
mathjax: true
author:     "zwt"
header-img: "img/post-bg-2015.jpg"
catalog: false
tags:
    - 工具
---
* TOC
{:toc}
# 一些操作
```
1.list转tensor：torch.Tensor(list)
2.tensor 转list：tensor.numpy().tolist()
3.tensor转numpy:tensor.cpu().numpy
4.# 限制0号设备的显存的使用量为0.5，就是半张卡那么多，比如12G卡，设置0.5就是6G。
torch.cuda.set_per_process_memory_fraction(0.5, 0)
torch.cuda.empty_cache()
```
# 交叉熵损失函数
交叉熵主要是用来判定实际的输出与期望的输出的接近程度。
## 一般实现
举个例子：假如小明和小王去打靶，那么打靶结果其实是一个0-1分布，X的取值有{0：打中，1：打不中}。在打靶之前我们知道小明和小王打中的先验概率为10%，99.9%。根据上面的信息我们可以分别得到小明和小王打靶打中的信息量。但是如果我们想进一步度量小明打靶结果的不确定度，这就需要用到熵的概念了。那么如何度量呢，那就要采用期望了。我们对所有可能事件所带来的信息量求期望，其结果就能衡量小明打靶的不确定度：
$$
H_A(x)=-\left[p\left(x_A\right) \log \left(p\left(x_A\right)\right)+\left(1-p\left(x_A\right)\right) \log \left(1-p\left(x_A\right)\right)\right]=0.4690
$$
与之对应的，小王的熵（打靶的不确定度）为：
$$
H_B(x)=-\left[p\left(x_B\right) \log \left(p\left(x_B\right)\right)+\left(1-p\left(x_B\right)\right) \log \left(1-p\left(x_B\right)\right)\right]=0.0114
$$
交叉熵：它主要刻画的是实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，两个概率分布就越接近。假设概率分布p为期望输出，概率分布q为实际输出， H(p,q)为交叉熵，则:
$$
H(p, q)=-\sum_x(p(x) \log q(x)+(1-p(x)) \log (1-q(x)))
$$

## torch中实现
Pytorch中CrossEntropyLoss()函数的主要是将softmax-log-NLLLoss合并到一块得到的结果
$$
\operatorname{Loss}(x, \text { class })=-\log \left(\frac{e^{x[\text { class }]}}{\sum_i e^{x[i]}}\right)=-x[\text { class }]+\log \left(\sum_i e^{x[i]}\right)
$$


# 参考
1.[交叉熵](https://zhuanlan.zhihu.com/p/98785902)
2.[torch交叉熵](https://mdnice.com/writing/6ed27b611add443b9fa5717fe286971e)
3.[torch交叉熵](https://www.jb51.net/article/274051.htm)