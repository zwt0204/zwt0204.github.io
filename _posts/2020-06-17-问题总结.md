---
layout:     post
title:      "问题总结"
subtitle:   " \"面试问题\""
date:       2020-06-17 18:00:00
mathjax: true
author:     "zwt"
header-img: "img/post-bg-2015.jpg"
catalog: false
tags:
    - question
---
* TOC
{:toc}
# 机器学习理论
1.  写出全概率公式&贝叶斯公式
2.  模型训练为什么要引入偏差(bias)和方差(variance)？  证
3.  CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型
4.  如何解决过拟合问题？
5.  One-hot的作用是什么？为什么不直接使用数字作为表示
6.  决策树和随机森林的区别是什么？
7.  朴素贝叶斯为什么“朴素naive”？
8.  kmeans初始点除了随机选取之外的方法
9.  LR明明是分类模型为什么叫回归
10. 梯度下降如何并行化
11. 梯度下降如何并行化
12. LR中的L1/L2正则项是啥
13. 简述决策树构建过程
14. 解释Gini系数
15. 决策树的优缺点
16. 出现估计概率值为 0 怎么处理
17. 随机森林的生成过程
18. 介绍一下Boosting的思想
19. gbdt的中的tree是什么tree？有什么特征
20. xgboost对比gbdt/boosting Tree有了哪些方向上的优化
21. 什么叫最优超平面
22. 什么是支持向量
23. SVM如何解决多分类问题
24. 核函数的作用是啥

# 深度学习
1.  你觉得batch-normalization过程是什么样的
2.  激活函数有什么用？常见的激活函数的区别是什么？
3.  Softmax的原理是什么？有什么作用？
CNN的平移不变性是什么？如何实现的？
4.  VGG，GoogleNet，ResNet等网络之间的区别是什么？
5.  残差网络为什么能解决梯度消失的问题
6.  LSTM为什么能解决梯度消失/爆炸的问题
7.  Attention对比RNN和CNN，分别有哪点你觉得的优势
8.  写出Attention的公式
9.  Attention机制，里面的q,k,v分别代表什么
10. 为什么self-attention可以替代seq2seq

# NLP
1.  GolVe的损失函数
2.  为什么GolVe会用的相对比W2V少
3.  层次softmax流程
4.  负采样流程
5.  怎么衡量学到的embedding的好坏
6.  阐述CRF原理
7.  详述LDA原理
8.  LDA中的主题矩阵如何计算
9.  LDA和Word2Vec区别？LDA和Doc2Vec区别
10. Bert的双向体现在什么地方
11. Bert的是怎样预训练的
12. 在数据中随机选择 15% 的标记，其中80%被换位[mask]，10%不变、10%随机替换其他单词，原因是什么
13. 为什么BERT有3个嵌入层，它们都是如何实现的
14. 手写一个multi-head attention

# 图像
1.  常见的模型加速方法
2.  目标检测里如何有效解决常见的前景少背景多的问题
3.  目标检测里有什么情况是SSD、YOLOv3、Faster R-CNN等所不能解决的，假设网络拟合能力无限强
4.  ROIPool和ROIAlign的区别
5.  介绍常见的梯度下降优化方法
6.  Detection你觉的还有哪些可做的点
7.  mini-Batch SGD相对于GD有什么优点
8.  人体姿态估计主流的两个做法是啥？简单介绍下
9.  卷积的实现原理以及如何快速高效实现局部weight sharing的卷积操作方式
10. CycleGAN的生成效果为啥一般都是位置不变纹理变化，为啥不能产生不同位置的生成效果

# 推荐系统
1.  DNN与DeepFM之间的区别
2.  你在使用deepFM的时候是如何处理欠拟合和过拟合问题的
3.  deepfm的embedding初始化有什么值得注意的地方吗
4.  YoutubeNet 变长数据如何处理的
5.  YouTubeNet如何避免百万量级的softmax问题的
6.  推荐系统有哪些常见的评测指标？
7.  MLR的原理是什么？做了哪些优化？