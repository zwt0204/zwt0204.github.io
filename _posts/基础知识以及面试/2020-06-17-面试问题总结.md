---
layout:     post
title:      "问题总结"
subtitle:   " \"面试问题\""
date:       2020-06-17 18:00:00
mathjax: true
author:     "zwt"
header-img: "img/post-bg-2015.jpg"
catalog: false
tags:
    - question
---
* TOC
{:toc}

# 公司面试问题
## 微盟
大数找中位数，内存有限：
```
对数据进行分段：整数的取值范围是-2（31）-2（31）-1
将其分组到100000，每一组43000个数，最后一组少一个数。然后分次载入数据，数据落入哪一个区间，哪一个区间就+1，遍历完后，我们得到数据落入某个区间的统计值。
现在需要确定中位数在哪个区间，累加每个区间的统计值，在统计值达到中位数要求时，就得到了最后的结果。
```

传统的crf与bilstm中的crf有什么区别：
```
传统的crf需要定义特征函数，bilstm+crf中特征函数通过bilstm抽取得到。
传统的crf计算损失是通过对相应的特征函数加权求和归一化得到的，bilstm+crf计算损失的时候是计算真实路径的分数与所有路径分数的比值求log。其中真实路径的分数是转移分数+状态分数
```

聚类和分类的区别：
```
聚类目标不明确，不知道数据可以分为多少类别，所以一般是一种无监督学习
分类事先知道类别个数，通常是做有监督学习
```

knn与k-means的区别：
```
knn:没有显示的学习的过程，事先已经有了特征和标签，来了一个新的样本，计算样本与原始数据的距离，通过投票法来得出当前新样本是那个类别。
	步骤：
	首先，随机初始化k个初始点作为质心
	然后将数据集中的每一个点分配到一个簇中，即为每一个点找到距离其最近的质心，并将其分配给该质心所对应的簇
	然后将每一个质心用簇中所有点的均值进行更新
kmeans：属于一种无监督学习，需要一个学习的过程。
```

## 平安
svm中的ktt条件：
```
在含有不等式约束的时候，转化为满足kkt条件下应用拉格朗日乘子法
```

常用的机器学习深度学习分类算法，有什么优缺点以及相应的适用场景：
```
机器学习：SVM：
深度学习：bilstm：
```

短文本分类用什么算法好：
```
个人理解，Textcnn：因为其捕获的信息是n-gram特征，短文本通常词具有带变形
```

## 一览群智
知识图谱推理问题：
```
主要是要了解相应的包以及构建知识图谱时本体层的构建
```

decoder解码的优势：
```
速度快
```

fasttext为什么快：
```
最后一层采用分层softmax
```

# 网络收集问题

## 机器学习理论
1.  写出全概率公式&贝叶斯公式
2.  模型训练为什么要引入偏差(bias)和方差(variance)？  证
3.  CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型
4.  如何解决过拟合问题？
5.  One-hot的作用是什么？为什么不直接使用数字作为表示
6.  决策树和随机森林的区别是什么？
7.  朴素贝叶斯为什么“朴素naive”？
8.  kmeans初始点除了随机选取之外的方法
9.  LR明明是分类模型为什么叫回归
10. 梯度下降如何并行化
11. 梯度下降如何并行化
12. LR中的L1/L2正则项是啥
13. 简述决策树构建过程
14. 解释Gini系数
15. 决策树的优缺点
16. 出现估计概率值为 0 怎么处理
17. 随机森林的生成过程
18. 介绍一下Boosting的思想
19. gbdt的中的tree是什么tree？有什么特征
20. xgboost对比gbdt/boosting Tree有了哪些方向上的优化
21. 什么叫最优超平面
22. 什么是支持向量
23. SVM如何解决多分类问题
24. 核函数的作用是啥

## 深度学习
1.  你觉得batch-normalization过程是什么样的
2.  激活函数有什么用？常见的激活函数的区别是什么？
3.  Softmax的原理是什么？有什么作用？
4.  VGG，GoogleNet，ResNet等网络之间的区别是什么？
5.  残差网络为什么能解决梯度消失的问题
6.  LSTM为什么能解决梯度消失/爆炸的问题
7.  Attention对比RNN和CNN，分别有哪点你觉得的优势
8.  写出Attention的公式
9.  Attention机制，里面的q,k,v分别代表什么
10. 为什么self-attention可以替代seq2seq
11. CNN的平移不变性是什么？如何实现的？

## NLP
1.  GolVe的损失函数
2.  为什么GolVe会用的相对比W2V少
3.  层次softmax流程
4.  负采样流程
5.  怎么衡量学到的embedding的好坏
6.  阐述CRF原理
7.  详述LDA原理
8.  LDA中的主题矩阵如何计算
9.  LDA和Word2Vec区别？LDA和Doc2Vec区别
10. Bert的双向体现在什么地方
11. Bert的是怎样预训练的
12. 在数据中随机选择 15% 的标记，其中80%被换位[mask]，10%不变、10%随机替换其他单词，原因是什么
13. 为什么BERT有3个嵌入层，它们都是如何实现的
14. 手写一个multi-head attention

## 图像
1.  常见的模型加速方法
2.  目标检测里如何有效解决常见的前景少背景多的问题
3.  目标检测里有什么情况是SSD、YOLOv3、Faster R-CNN等所不能解决的，假设网络拟合能力无限强
4.  ROIPool和ROIAlign的区别
5.  介绍常见的梯度下降优化方法
6.  Detection你觉的还有哪些可做的点
7.  mini-Batch SGD相对于GD有什么优点
8.  人体姿态估计主流的两个做法是啥？简单介绍下
9.  卷积的实现原理以及如何快速高效实现局部weight sharing的卷积操作方式
10. CycleGAN的生成效果为啥一般都是位置不变纹理变化，为啥不能产生不同位置的生成效果

## 推荐系统
1.  DNN与DeepFM之间的区别
2.  你在使用deepFM的时候是如何处理欠拟合和过拟合问题的
3.  deepfm的embedding初始化有什么值得注意的地方吗
4.  YoutubeNet 变长数据如何处理的
5.  YouTubeNet如何避免百万量级的softmax问题的
6.  推荐系统有哪些常见的评测指标？
7.  MLR的原理是什么？做了哪些优化？

## 代码
### 自注意力
```python
def self_attention(query, key, value, dropout=None, mask=None):
	# 取最后一维嵌入维度
    d_k = query.size(-1)
    # 转置key的后两个维度和query计算score
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    # mask的操作在QK之后，softmax之前
    if mask is not None:
        mask.cuda()
        scores = scores.masked_fill(mask == 0, -1e9)
    self_attn = F.softmax(scores, dim=-1)
    if dropout is not None:
        self_attn = dropout(self_attn)
    return torch.matmul(self_attn, value), self_attn
```
### 多头注意力
```python
class MultiHeadAttention(nn.Module):
    def __init__(self):
        super(MultiHeadAttention, self).__init__()

    def forward(self,  head, d_model, query, key, value, dropout=0.1,mask=None):
        """
        :param head: 头数，默认 8
        :param d_model: 输入的维度 512
        :param query: Q
        :param key: K
        :param value: V
        :param dropout:
        :param mask:
        :return:
        """
        assert (d_model % head == 0)
        # 除不尽的话可以考虑填充
        self.d_k = d_model // head
        self.head = head
        self.d_model = d_model
		# w_q,w_k,w_v线性层
        self.linear_query = nn.Linear(d_model, d_model)
        self.linear_key = nn.Linear(d_model, d_model)
        self.linear_value = nn.Linear(d_model, d_model)
        # 自注意力机制的 QKV 同源，线性变换
        self.linear_out = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(p=dropout)
        self.attn = None
        # if mask is not None:
        #     # 多头注意力机制的线性变换层是4维，是把query[batch, frame_num, d_model]变成[batch, -1, head, d_k]
        #     # 再1，2维交换变成[batch, head, -1, d_k], 所以mask要在第一维添加一维，与后面的self attention计算维度一样
        #     mask = mask.unsqueeze(1)
        n_batch = query.size(0)
        # 多头需要对这个 X 切分成多头
        # query==key==value
        # [b,1,512]
        # [b,8,1,64]

        # [b,32,512]
        # [b,8,32,64]
		
		# head 也相当于batch
        query = self.linear_query(query).view(n_batch, -1, self.head, self.d_k).transpose(1, 2)  # [b, 8, 32, 64]
        key = self.linear_key(key).view(n_batch, -1, self.head, self.d_k).transpose(1, 2)  # [b, 8, 32, 64]
        value = self.linear_value(value).view(n_batch, -1, self.head, self.d_k).transpose(1, 2)  # [b, 8, 32, 64]
        x, self.attn = self_attention(query, key, value, dropout=self.dropout, mask=mask)
        # [b,8,32,64]
        # [b,32,512]
        # 变为三维， 或者说是concat head
        x = x.transpose(1, 2).contiguous().view(n_batch, -1, self.head * self.d_k)
        return self.linear_out(x)
```