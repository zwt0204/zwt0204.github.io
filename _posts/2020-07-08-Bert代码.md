---
layout:     post
title:      "bert"
subtitle:   " \"bert\""
date:       2020-07-01 15:30:00 
mathjax: true
author:     "zwt"
header-img: "img/post-bg-2015.jpg"
catalog: false
tags:
    - bert
---
* TOC
{:toc}
# 准备

```
英文版本有四个，但是中文的目前只有一个版本BERT_base版本
12层，768个隐藏单元，12个注意力头，110M参数
同时还有包含104中语言，12层，768隐藏单元，12注意力头，110M参数
```
[下载地址](https://github.com/google-research/bert#pre-trained-models)

[google代码](https://github.com/google-research/bert)

下载下来文件的解释：
```
vocab.txt是模型的词典
bert_config.json是超参数的配置
bert_model.ckpt.*预训练好的模型
```

环境：
```
tensorflow1.12+
```

# modeling.py
## BertConfig是为加载配置文件所定义的对象
```
vocab_size,  # 输入的词典词数量
hidden_size=768,  # 隐藏单元数 
num_hidden_layers=12,  # 堆叠层数
num_attention_heads=12,  # 多头注意力的头数
intermediate_size=3072,  # 前向传播的layer大小
hidden_act="gelu",  # 激活函数
hidden_dropout_prob=0.1,  # 全连接层和pooler层的dropout
attention_probs_dropout_prob=0.1,  # 乘法attention时，softmax后dropout概率
max_position_embeddings=512,  # 输入句子的最大长度
type_vocab_size=2,  # Segment A和 Segment B
initializer_range=0.02:  # 随机初始化正态分布的参数
```
## BertModel对象：
```python

class BertModel(object):
    """BERT model ("Bidirectional Encoder Representations from Transformers").
    Example usage:

    # Already been converted into WordPiece token ids
    input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])
    # 1表示原始有token，0表示是padding出来的
    input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])
    token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])

    config = modeling.BertConfig(vocab_size=32000, hidden_size=512,
      num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)

    model = modeling.BertModel(config=config, is_training=True,
      input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)

    label_embeddings = tf.get_variable(...)
    pooled_output = model.get_pooled_output()
    logits = tf.matmul(pooled_output, label_embeddings)
    """
	    def __init__(self,
                 config,  # BertConfig对象
                 is_training,  # 辨识是否为训练，影响dropout
                 input_ids,  # int32 Tensor  shape是[batch_size, seq_length]
                 input_mask=None,  # (可选) int32 Tensor shape是[batch_size, seq_length]
                 token_type_ids=None,  # (可选) int32 Tensor shape是[batch_size, seq_length]
                 use_one_hot_embeddings=False,  # (可选) bool
                 # 如果True，使用矩阵乘法实现提取词的Embedding；否则用tf.embedding_lookup()
                 # 对于TPU，使用前者更快，对于GPU和CPU，后者更快。
                 scope=None):
        config = copy.deepcopy(config)
        if not is_training:
            config.hidden_dropout_prob = 0.0
            config.attention_probs_dropout_prob = 0.0

        input_shape = get_shape_list(input_ids, expected_rank=2)
        batch_size = input_shape[0]
        seq_length = input_shape[1]

        if input_mask is None:
            input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)

        if token_type_ids is None:
            token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)

        # 管理传给get_variable()的变量名称的作用域
        with tf.variable_scope(scope, default_name="bert"):
            with tf.variable_scope("embeddings"):
                # 词的Embedding lookup
                (self.embedding_output, self.embedding_table) = embedding_lookup(
                    input_ids=input_ids,
                    vocab_size=config.vocab_size,
                    embedding_size=config.hidden_size,
                    initializer_range=config.initializer_range,
                    word_embedding_name="word_embeddings",
                    use_one_hot_embeddings=use_one_hot_embeddings)

                # 增加位置embeddings和token type的embeddings，然后是
                # layer normalize和dropout。
                self.embedding_output = embedding_postprocessor(
                    input_tensor=self.embedding_output,
                    use_token_type=True,
                    token_type_ids=token_type_ids,
                    token_type_vocab_size=config.type_vocab_size,
                    token_type_embedding_name="token_type_embeddings",
                    use_position_embeddings=True,
                    position_embedding_name="position_embeddings",
                    initializer_range=config.initializer_range,
                    max_position_embeddings=config.max_position_embeddings,
                    dropout_prob=config.hidden_dropout_prob)

            with tf.variable_scope("encoder"):
                # 把shape为[batch_size, seq_length]的2D mask变成
                # shape为[batch_size, seq_length, seq_length]的3D mask
                # 以便后向的attention计算
                attention_mask = create_attention_mask_from_input_mask(
                    input_ids, input_mask)

                # 多个Transformer模型stack起来。
                # all_encoder_layers是一个list，长度为num_hidden_layers（默认12），每一层对应一个值。
                # 每一个值都是一个shape为[batch_size, seq_length, hidden_size]的tensor。
                self.all_encoder_layers = transformer_model(
                    input_tensor=self.embedding_output,
                    attention_mask=attention_mask,
                    hidden_size=config.hidden_size,
                    num_hidden_layers=config.num_hidden_layers,
                    num_attention_heads=config.num_attention_heads,
                    intermediate_size=config.intermediate_size,
                    intermediate_act_fn=get_activation(config.hidden_act),
                    hidden_dropout_prob=config.hidden_dropout_prob,
                    attention_probs_dropout_prob=config.attention_probs_dropout_prob,
                    initializer_range=config.initializer_range,
                    do_return_all_layers=True)
            # `sequence_output` 是最后一层的输出，shape是[batch_size, seq_length, hidden_size]
            self.sequence_output = self.all_encoder_layers[-1]

            with tf.variable_scope("pooler"):
                # 取最后一层的第一个时刻[CLS]对应的tensor
                # 从[batch_size, seq_length, hidden_size]变成[batch_size, hidden_size]
                # sequence_output[:, 0:1, :]得到的是[batch_size, 1, hidden_size]
                # 我们需要用squeeze把第二维去掉。
                # tf.squeeze()函数的作用是从tensor中删除所有大小(szie)是1的维度
                first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)
                # 然后再加一个全连接层，输出仍然是[batch_size, hidden_size]
                self.pooled_output = tf.layers.dense(
                    first_token_tensor,
                    config.hidden_size,
                    activation=tf.tanh,
                    kernel_initializer=create_initializer(config.initializer_range))

    def get_pooled_output(self):
        return self.pooled_output

    def get_sequence_output(self):
        """Gets final hidden layer of encoder.

        Returns:
          float Tensor of shape [batch_size, seq_length, hidden_size] corresponding
          to the final hidden of the transformer encoder.
        """
        return self.sequence_output

    def get_all_encoder_layers(self):
        return self.all_encoder_layers

    def get_embedding_output(self):
        """Gets output of the embedding lookup (i.e., input to the transformer).

        Returns:
          float Tensor of shape [batch_size, seq_length, hidden_size] corresponding
          to the output of the embedding layer, after summing the word
          embeddings with the positional embeddings and the token type embeddings,
          then performing layer normalization. This is the input to the transformer.
        """
        return self.embedding_output

    def get_embedding_table(self):
        return self.embedding_table
```
## gelu
```python
"""Gaussian Error Linear Unit.

    This is a smoother version of the RELU.
    Original paper: https://arxiv.org/abs/1606.08415
    Args:
      x: float Tensor to perform activation.

    Returns:
      `x` with the GELU activation applied.
    """
    cdf = 0.5 * (1.0 + tf.tanh(
        (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))
    return x * cdf
```
## embedding_lookup
```python
def embedding_lookup(input_ids,  # int32 Tensor shape为[batch_size, seq_length]，表示WordPiece的id
                     vocab_size,  # 词典大小
                     embedding_size=128,  # embedding后向量的大小
                     initializer_range=0.02,  # 随机初始化的范围
                     word_embedding_name="word_embeddings",
                     use_one_hot_embeddings=False):
    # 这个函数假设输入的shape是[batch_size, seq_length, num_inputs]
    # 普通的Embeding一般假设输入是[batch_size, seq_length]，
    # 增加num_inputs这一维度的目的是为了一次计算更多的Embedding
    # 但目前的代码并没有用到，传入的input_ids都是2D的，这增加了代码的阅读难度。

    # 如果输入是[batch_size, seq_length]，
    # 那么我们把它 reshape成[batch_size, seq_length, 1]
    if input_ids.shape.ndims == 2:
        input_ids = tf.expand_dims(input_ids, axis=[-1])
    # 构造Embedding矩阵，shape是[vocab_size, embedding_size]
    embedding_table = tf.get_variable(
        name=word_embedding_name,
        shape=[vocab_size, embedding_size],
        initializer=create_initializer(initializer_range))

    flat_input_ids = tf.reshape(input_ids, [-1])
    if use_one_hot_embeddings:
        one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)
        output = tf.matmul(one_hot_input_ids, embedding_table)
    else:
        output = tf.gather(embedding_table, flat_input_ids)

    input_shape = get_shape_list(input_ids)
    # 把输出从[batch_size, seq_length, num_inputs(这里总是1), embedding_size]
    # 变成[batch_size, seq_length, num_inputs*embedding_size]
    output = tf.reshape(output,
                        input_shape[0:-1] + [input_shape[-1] * embedding_size])
    return (output, embedding_table)
```
## embedding_postprocessor
```python
def embedding_postprocessor(input_tensor,  # shape为[batch_size, seq_length, embedding_size]
                            use_token_type=False,  # 是否增加`token_type_ids`的Embedding 当前词隶属那个句子
                            token_type_ids=None,  # shape为[batch_size, seq_length] 如果`use_token_type`为True则必须有值
                            token_type_vocab_size=16,  # int Token Type的个数，通常是2
                            token_type_embedding_name="token_type_embeddings",
                            use_position_embeddings=True,  # 是否使用位置Embedding
                            position_embedding_name="position_embeddings",  # 位置embedding的名字
                            initializer_range=0.02,  # 初始化范围
                            max_position_embeddings=512,  # 位置编码的最大长度，可以比最大序列长度大，但是不能比它小。
                            dropout_prob=0.1):
    input_shape = get_shape_list(input_tensor, expected_rank=3)
    batch_size = input_shape[0]
    seq_length = input_shape[1]
    width = input_shape[2]

    output = input_tensor

    if use_token_type:
        if token_type_ids is None:
            raise ValueError("`token_type_ids` must be specified if"
                             "`use_token_type` is True.")
        token_type_table = tf.get_variable(
            name=token_type_embedding_name,
            shape=[token_type_vocab_size, width],
            initializer=create_initializer(initializer_range))
        # 因为Token Type通常很小(2)，所以直接用矩阵乘法(one-hot)更快
        flat_token_type_ids = tf.reshape(token_type_ids, [-1])
        one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)
        token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)
        token_type_embeddings = tf.reshape(token_type_embeddings,
                                           [batch_size, seq_length, width])
        output += token_type_embeddings

    if use_position_embeddings:
        # x 大于y将会抛出异常
        assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)
        with tf.control_dependencies([assert_op]):
            full_position_embeddings = tf.get_variable(
                name=position_embedding_name,
                shape=[max_position_embeddings, width],
                initializer=create_initializer(initializer_range))
            # 位置Embedding是可以学习的参数，因此我们创建一个[max_position_embeddings, width]的矩阵
            # 但实际输入的序列可能并不会到max_position_embeddings(512)，为了提高训练速度，
            # 我们通过tf.slice取出[0, 1, 2, ..., seq_length-1]的部分。
            # 因此我们需要扩展位置编码为[1, seq_length, width]
            # 然后就能通过broadcasting加上去了。
            # 从full_position_embeddings的[0,0]也就是第0个位置开始，第一个维度取seq_length个元素，第二个全取
            position_embeddings = tf.slice(full_position_embeddings, [0, 0],
                                           [seq_length, -1])
            num_dims = len(output.shape.as_list())

            position_broadcast_shape = []
            for _ in range(num_dims - 2):
                position_broadcast_shape.append(1)
            position_broadcast_shape.extend([seq_length, width])
            # 默认情况下position_broadcast_shape为[1, 128, 768]
            position_embeddings = tf.reshape(position_embeddings,
                                             position_broadcast_shape)
            # output是[8, 128, 768], position_embeddings是[1, 128, 768]
            # 因此可以通过broadcasting相加。
            output += position_embeddings

    output = layer_norm_and_dropout(output, dropout_prob)
    return output

```
## create_attention_mask_from_input_mask
```python
def create_attention_mask_from_input_mask(from_tensor, to_mask):
    """
      input_ids=[[1,2,3,0,0],[1,3,5,6,1]]
     input_mask=[[1,1,1,0,0],[1,1,1,1,1]]
     比如:broadcast_ones的shape是[2, 5, 1]，值全是1，而to_mask是[[1,1,1,0,0],[1,1,1,1,1]]
     shape是[2, 5]，reshape为[2, 1, 5]。然后broadcast_ones * to_mask就得到[2, 5, 5]，
     正是我们需要的两个Mask矩阵，可以验证。注意[batch, A, B]*[batch, B, C]=[batch, A, C]，
     我们可以认为是batch个[A, B]的矩阵乘以batch个[B, C]的矩阵。
     [
        [1, 1, 1, 0, 0], #它表示第1个词可以attend to 3个词
        [1, 1, 1, 0, 0], #它表示第2个词可以attend to 3个词
        [1, 1, 1, 0, 0], #它表示第3个词可以attend to 3个词
        [1, 1, 1, 0, 0], #无意义，因为输入第4个词是padding的0
        [1, 1, 1, 0, 0]  #无意义，因为输入第5个词是padding的0
    ]

    [
        [1, 1, 1, 1, 1], # 它表示第1个词可以attend to 5个词
        [1, 1, 1, 1, 1], # 它表示第2个词可以attend to 5个词
        [1, 1, 1, 1, 1], # 它表示第3个词可以attend to 5个词
        [1, 1, 1, 1, 1], # 它表示第4个词可以attend to 5个词
        [1, 1, 1, 1, 1]	 # 它表示第5个词可以attend to 5个词
    ]
    """
    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])
    batch_size = from_shape[0]
    from_seq_length = from_shape[1]

    to_shape = get_shape_list(to_mask, expected_rank=2)
    to_seq_length = to_shape[1]

    to_mask = tf.cast(
        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)

    broadcast_ones = tf.ones(
        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)

    # Here we broadcast along two dimensions to create the mask.
    mask = broadcast_ones * to_mask

    return mask
```
## attention_layer
```python
"""
这个函数实现论文"Attention
is all you Need"里的multi-head attention。
如果`from_tensor`和`to_tensor`是同一个tensor，那么就实现Self-Attention。
`from_tensor`的每个时刻都会attends to `to_tensor`，
也就是用from的Query去乘以所有to的Key，得到weight，然后把所有to的Value加权求和起来。
这个函数首先把`from_tensor`变换成一个"query" tensor，
然后把`to_tensor`变成"key"和"value" tensors。
总共有`num_attention_heads`组Query、Key和Value，
每一个Query，Key和Value的shape都是[batch_size(8), seq_length(128), size_per_head(512/8=64)].
然后计算query和key的内积并且除以size_per_head的平方根(8)。
然后softmax变成概率，最后用概率加权value得到输出。
因为有多个Head，每个Head都输出[batch_size, seq_length, size_per_head]，
最后把8个Head的结果concat起来，就最终得到[batch_size(8), seq_length(128), size_per_head*8=512] 
实际上我们是把这8个Head的Query，Key和Value都放在一个Tensor里面的，
因此实际通过transpose和reshape就达到了上面的效果。
"""


def attention_layer(from_tensor,  # shape [batch_size, from_seq_length, from_width]
                    to_tensor,  # shape [batch_size, to_seq_length, to_width].
                    attention_mask=None,  # shape[batch_size,from_seq_length,to_seq_length]。值可以是0或者1，
                    # 在计算attention score的时候，我们会把0变成负无穷(实际是一个绝对值很大的负数)，而1不变，
                    # 这样softmax的时候进行exp的计算，前者就趋近于零，从而间接实现Mask的功能。
                    num_attention_heads=1,  # Attention heads的数量。
                    size_per_head=512,  # 每个head的size
                    query_act=None,  # query变换的激活函数
                    key_act=None,  # key变换的激活函数
                    value_act=None,  # value变换的激活函数
                    attention_probs_dropout_prob=0.0,  # attention的Dropout概率
                    initializer_range=0.02,
                    do_return_2d_tensor=False,
                    # 如果True，返回2D的Tensor其shape是[batch_size * from_seq_length, num_attention_heads * size_per_head]；
                    # 否则返回3D的Tensor其shape为[batch_size, from_seq_length, num_attention_heads * size_per_head].
                    batch_size=None,  # 如果输入是3D的，那么batch就是第一维，但是可能3D的压缩成了2D的，所以需要告诉函数batch_size
                    from_seq_length=None,  # 需要告诉函数from_seq_length
                    to_seq_length=None):  # 同上，to_seq_length
    # float Tensor，shape [batch_size,from_seq_length,num_attention_heads * size_per_head]。
    # 如果`do_return_2d_tensor`为True，则返回的shape是
    # [batch_size * from_seq_length, num_attention_heads * size_per_head].

    def transpose_for_scores(input_tensor, batch_size, num_attention_heads,
                             seq_length, width):
        output_tensor = tf.reshape(
            input_tensor, [batch_size, seq_length, num_attention_heads, width])

        output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])
        return output_tensor

    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])
    to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])

    if len(from_shape) != len(to_shape):
        raise ValueError(
            "The rank of `from_tensor` must match the rank of `to_tensor`.")
    # 如果输入是3D的(没有压缩)，那么我们可以推测出batch_size、from_seq_length和to_seq_length
    # 即使参数传入也会被覆盖。
    if len(from_shape) == 3:
        batch_size = from_shape[0]
        from_seq_length = from_shape[1]
        to_seq_length = to_shape[1]
    # 如果是压缩成2D的，那么一定要传入这3个参数，否则抛异常。
    elif len(from_shape) == 2:
        if (batch_size is None or from_seq_length is None or to_seq_length is None):
            raise ValueError(
                "When passing in rank 2 tensors to attention_layer, the values "
                "for `batch_size`, `from_seq_length`, and `to_seq_length` "
                "must all be specified.")
    #   B = batch size (number of sequences) 默认配置是8
    #   F = `from_tensor` sequence length 默认配置是128
    #   T = `to_tensor` sequence length 默认配置是128
    #   N = `num_attention_heads` 默认配置是12
    #   H = `size_per_head` 默认配置是64

    # 把from和to压缩成2D的。
    # [8*128, 768]
    from_tensor_2d = reshape_to_matrix(from_tensor)
    # [8*128, 768]
    to_tensor_2d = reshape_to_matrix(to_tensor)

    # 计算Query `query_layer` = [B*F, N*H] =[8*128, 12*64]
    # batch_size=8，共128个时刻，12和head，每个head的query向量是64
    # 因此最终得到[8*128, 12*64]
    query_layer = tf.layers.dense(
        from_tensor_2d,
        num_attention_heads * size_per_head,
        activation=query_act,
        name="query",
        kernel_initializer=create_initializer(initializer_range))

    # 和query类似，`key_layer` = [B*T, N*H]
    key_layer = tf.layers.dense(
        to_tensor_2d,
        num_attention_heads * size_per_head,
        activation=key_act,
        name="key",
        kernel_initializer=create_initializer(initializer_range))

    # 同上，`value_layer` = [B*T, N*H]
    value_layer = tf.layers.dense(
        to_tensor_2d,
        num_attention_heads * size_per_head,
        activation=value_act,
        name="value",
        kernel_initializer=create_initializer(initializer_range))

    # 把query从[B*F, N*H] =[8*128, 12*64]变成[B, N, F, H]=[8, 12, 128, 64]
    query_layer = transpose_for_scores(query_layer, batch_size,
                                       num_attention_heads, from_seq_length,
                                       size_per_head)

    # 同上，key也变成[8, 12, 128, 64]
    key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,
                                     to_seq_length, size_per_head)

    # 计算query和key的内积，得到attention scores.
    # [8, 12, 128, 64]*[8, 12, 64, 128]=[8, 12, 128, 128]
    # 最后两维[128, 128]表示from的128个时刻attend to到to的128个score。
    # `attention_scores` = [B, N, F, T]
    attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)
    attention_scores = tf.multiply(attention_scores,
                                   1.0 / math.sqrt(float(size_per_head)))

    if attention_mask is not None:
        # 从[8, 128, 128]变成[8, 1, 128, 128]
        # `attention_mask` = [B, 1, F, T]
        attention_mask = tf.expand_dims(attention_mask, axis=[1])

        # 这个小技巧前面也用到过，如果mask是1，那么(1-1)*-10000=0，adder就是0,
        # 如果mask是0，那么(1-0)*-10000=-10000。
        adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0

        # 我们把adder加到attention_score里，mask是1就相当于加0，mask是0就相当于加-10000。
        # 通常attention_score都不会很大，因此mask为0就相当于把attention_score设置为负无穷
        # 后面softmax的时候就趋近于0，因此相当于不能attend to Mask为0的地方。
        attention_scores += adder

    # softmax
    # `attention_probs` = [B, N, F, T] =[8, 12, 128, 128]
    attention_probs = tf.nn.softmax(attention_scores)

    # 对attention_probs进行dropout，这虽然有点奇怪，但是Transformer的原始论文就是这么干的。
    attention_probs = dropout(attention_probs, attention_probs_dropout_prob)

    # 把`value_layer` reshape成[B, T, N, H]=[8, 128, 12, 64]
    value_layer = tf.reshape(
        value_layer,
        [batch_size, to_seq_length, num_attention_heads, size_per_head])

    # `value_layer`变成[B, N, T, H]=[8, 12, 128, 64]
    value_layer = tf.transpose(value_layer, [0, 2, 1, 3])

    # 计算`context_layer` = [8, 12, 128, 128]*[8, 12, 128, 64]=[8, 12, 128, 64]=[B, N, F, H]
    context_layer = tf.matmul(attention_probs, value_layer)

    # `context_layer` 变换成 [B, F, N, H]=[8, 128, 12, 64]
    context_layer = tf.transpose(context_layer, [0, 2, 1, 3])

    if do_return_2d_tensor:
        # `context_layer` = [B*F, N*H]
        context_layer = tf.reshape(
            context_layer,
            [batch_size * from_seq_length, num_attention_heads * size_per_head])
    else:
        # `context_layer` = [B, F, N*H]
        context_layer = tf.reshape(
            context_layer,
            [batch_size, from_seq_length, num_attention_heads * size_per_head])

    return context_layer
    
```
## transformer_model
```python
def transformer_model(input_tensor,  # shape为[batch_size, seq_length, hidden_size]
                      attention_mask=None,  # shape [batch_size, seq_length, seq_length], 1表示可以attend to，0表示不能。
                      hidden_size=768,  # Transformer隐单元个数
                      num_hidden_layers=12,  # 有多少个SubLayer
                      num_attention_heads=12,  # Transformer Attention Head个数。
                      intermediate_size=3072,  # 全连接层的隐单元个数
                      intermediate_act_fn=gelu,
                      hidden_dropout_prob=0.1,  # Self-Attention层残差之前的Dropout概率
                      attention_probs_dropout_prob=0.1,  # attention的Dropout概率
                      initializer_range=0.02,
                      do_return_all_layers=False):  # 返回所有层的输出还是最后一层的输出
    # 如果do_return_all_layers True，返回最后一层的输出，是一个Tensor，shape为[batch_size, seq_length, hidden_size]；
    # 否则返回所有层的输出，是一个长度为num_hidden_layers的list，list的每一个元素都是[batch_size, seq_length, hidden_size]。
    if hidden_size % num_attention_heads != 0:
        raise ValueError(
            "The hidden size (%d) is not a multiple of the number of attention "
            "heads (%d)" % (hidden_size, num_attention_heads))
    # 因为最终要输出hidden_size，总共有num_attention_heads个Head，因此每个Head输出
    # 为hidden_size / num_attention_heads
    attention_head_size = int(hidden_size / num_attention_heads)
    input_shape = get_shape_list(input_tensor, expected_rank=3)
    batch_size = input_shape[0]
    seq_length = input_shape[1]
    input_width = input_shape[2]

    # 因为需要残差连接，我们需要把输入加到Self-Attention的输出，因此要求它们的shape是相同的。
    if input_width != hidden_size:
        raise ValueError("The width of the input tensor (%d) != hidden size (%d)" %
                         (input_width, hidden_size))

    # 为了避免在2D和3D之间来回reshape，我们统一把所有的3D Tensor用2D来表示。
    # 虽然reshape在GPU/CPU上很快，但是在TPU上却不是这样，这样做的目的是为了优化TPU
    # input_tensor是[8, 128, 768], prev_output是[8*128, 768]=[1024, 768]
    prev_output = reshape_to_matrix(input_tensor)

    all_layer_outputs = []
    for layer_idx in range(num_hidden_layers):
        # 每一层都有自己的variable scope
        with tf.variable_scope("layer_%d" % layer_idx):
            layer_input = prev_output
            # attention层
            with tf.variable_scope("attention"):
                attention_heads = []
                with tf.variable_scope("self"):
                    attention_head = attention_layer(
                        from_tensor=layer_input,
                        to_tensor=layer_input,
                        attention_mask=attention_mask,
                        num_attention_heads=num_attention_heads,
                        size_per_head=attention_head_size,
                        attention_probs_dropout_prob=attention_probs_dropout_prob,
                        initializer_range=initializer_range,
                        do_return_2d_tensor=True,
                        batch_size=batch_size,
                        from_seq_length=seq_length,
                        to_seq_length=seq_length)
                    attention_heads.append(attention_head)

                # attention_output = None
                if len(attention_heads) == 1:
                    attention_output = attention_heads[0]
                else:
                    # 如果有多个head，那么需要把多个head的输出concat起来
                    attention_output = tf.concat(attention_heads, axis=-1)

                # 使用线性变换把前面的输出变成`hidden_size`，然后再加上`layer_input`(残差连接)
                with tf.variable_scope("output"):
                    attention_output = tf.layers.dense(
                        attention_output,
                        hidden_size,
                        kernel_initializer=create_initializer(initializer_range))
                    attention_output = dropout(attention_output, hidden_dropout_prob)
                    # 残差连接再加上layer norm。
                    attention_output = layer_norm(attention_output + layer_input)

            # 全连接层
            with tf.variable_scope("intermediate"):
                intermediate_output = tf.layers.dense(
                    attention_output,
                    intermediate_size,
                    activation=intermediate_act_fn,
                    kernel_initializer=create_initializer(initializer_range))

            # 然后是用一个线性变换把大小变回`hidden_size`，这样才能加残差连接
            with tf.variable_scope("output"):
                layer_output = tf.layers.dense(
                    intermediate_output,
                    hidden_size,
                    kernel_initializer=create_initializer(initializer_range))
                layer_output = dropout(layer_output, hidden_dropout_prob)
                layer_output = layer_norm(layer_output + attention_output)
                prev_output = layer_output
                all_layer_outputs.append(layer_output)

    if do_return_all_layers:
        final_outputs = []
        for layer_output in all_layer_outputs:
            final_output = reshape_from_matrix(layer_output, input_shape)
            final_outputs.append(final_output)
        return final_outputs
    else:
        final_output = reshape_from_matrix(prev_output, input_shape)
        return final_output

```
# tokenization
主要的流程是清除无用字符、所欲的字符采用一种形式。然后先用BasicTokenizer（基于空格标点等）来进行基础分词，然后再利用WordpieceTokenizer进行分词。词典中有预留的unused token是为了处理文本中没有的特殊字符。
```python
def convert_to_unicode(text):
    """
    把字符串变成unicode的字符串。这是为了兼容Python2和Python3，因为Python3的str就是unicode，而Python2的str其实是bytearray，Python2却有一个专门的unicode类型。
    """
    if six.PY3:
        if isinstance(text, str):
            return text
        elif isinstance(text, bytes):
            return text.decode("utf-8", "ignore")
        else:
            raise ValueError("Unsupported string type: %s" % (type(text)))
    else:
        raise ValueError("Not running on Python2 or Python 3?")
        
def load_vocab(vocab_file):
    """加载词典"""
    # 生成有序的字典，按照插入的顺序排序
    vocab = collections.OrderedDict()
    index = 0
    with tf.gfile.GFile(vocab_file, "r") as reader:
        while True:
            token = convert_to_unicode(reader.readline())
            if not token:
                break
            token = token.strip()
            vocab[token] = index
            index += 1
    return vocab
```
# 使用
## pretrain

## classfication

#### convert_single_example
```python
def convert_single_example(ex_index, example, label_list, max_seq_length,
				tokenizer):
	"""把一个`InputExample`对象变成`InputFeatures`."""
	# label_map把label变成id，这个函数每个example都需要执行一次，其实是可以优化的。
	# 只需要在可以再外面执行一次传入即可。
	label_map = {}
	for (i, label) in enumerate(label_list):
		label_map[label] = i
	
	tokens_a = tokenizer.tokenize(example.text_a)
	tokens_b = None
	if example.text_b:
		tokens_b = tokenizer.tokenize(example.text_b)
	
	if tokens_b:
		# 如果有b，那么需要保留3个特殊Token[CLS], [SEP]和[SEP]
		# 如果两个序列加起来太长，就需要去掉一些。
		_truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)
	else:
		# 没有b则只需要保留[CLS]和[SEP]两个特殊字符
		# 如果Token太多，就直接截取掉后面的部分。
		if len(tokens_a) > max_seq_length - 2:
			tokens_a = tokens_a[0:(max_seq_length - 2)]
	
	# BERT的约定是：
	# (a) 对于两个序列：
	#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]
	#  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1
	# (b) 对于一个序列：
	#  tokens:   [CLS] the dog is hairy . [SEP]
	#  type_ids: 0     0   0   0  0     0 0
	#
	# 这里"type_ids"用于区分一个Token是来自第一个还是第二个序列
	# 对于type=0和type=1，模型会学习出两个Embedding向量。
	# 虽然理论上这是不必要的，因为[SEP]隐式的确定了它们的边界。
	# 但是实际加上type后，模型能够更加容易的知道这个词属于那个序列。
	#
	# 对于分类任务，[CLS]对应的向量可以被看成 "sentence vector"
	# 注意：一定需要Fine-Tuning之后才有意义
	tokens = []
	segment_ids = []
	tokens.append("[CLS]")
	segment_ids.append(0)
	for token in tokens_a:
		tokens.append(token)
		segment_ids.append(0)
    tokens.append("[SEP]")
    segment_ids.append(0)
	
	if tokens_b:
		for token in tokens_b:
			tokens.append(token)
			segment_ids.append(1)
		tokens.append("[SEP]")
		segment_ids.append(1)
	
	input_ids = tokenizer.convert_tokens_to_ids(tokens)
	
	# mask是1表示是"真正"的Token，0则是Padding出来的。在后面的Attention时会通过tricky的技巧让
	# 模型不能attend to这些padding出来的Token上。
	input_mask = [1] * len(input_ids)
	
	# padding使得序列长度正好等于max_seq_length
	while len(input_ids) < max_seq_length:
		input_ids.append(0)
		input_mask.append(0)
		segment_ids.append(0)
 
	label_id = label_map[example.label]
	
	feature = InputFeatures(
		input_ids=input_ids,
		input_mask=input_mask,
		segment_ids=segment_ids,
		label_id=label_id)
	return feature
```

## squad

