---
layout:     post
title:      "半监督学习"
subtitle:   " \"半监督\""
date:       2020-01-20 18:00:00
mathjax: true
author:     "zwt"
header-img: "img/post-bg-2015.jpg"
catalog: false
tags:
    - 半监督学习
---
* TOC
{:toc}

# 基础概念

## 香农熵

$$
H(X)=\sum_{x} P(x) \log _{2}[P(x)]
$$
变量的不确定性越大，熵就越大，把变量弄清楚所需要的信息量也就越大。

## 熵正则化

熵正则化：就是在似然函数的基础上，再加上香农熵，值得注意的是，似然 函数的范围是 已经标记的数据，香农熵的范围是未标记的数据。
$$
C = \sum_{i=1}^{l} \ln P\left(y_{i} \mid x_{i} ; \theta\right) + \lambda \sum_{x} P(x) \log _{2}[P(x)]
$$

## 类间的低密度分离

低密度分离的假设就是假设数据非黑即白，在两个类别的数据之间存在较为明显的鸿沟，即在两个类别之间的边界数据的密度很低（数据质量很好）

## Self-training

首先基于有标签的数据训练一个模型，然后将没有标签的数据输入进去进行预测，得到没有标签数据的一个标签，之后将一部分的带有伪标签的数据转移到有标签的数据中，再进行训练，循环往复。如何选取加入有标签数据的伪标签数据需要自己定义。

## 平滑性假设

数据的分布是不均匀的，有的地方比较稀疏，有的地方比较密集。
如果在高密度的地方比较相近，则两个数据具有相同的标签
精神：近朱者赤、近墨者黑

## Denosing Auto-Encoder

以一定的概率分布去擦除原始的input矩阵（这样看起来是丢失了部分数据）。以这丢失的数据去计算y，计算z，并将z和原始x做误差迭代，这样网络就学习了这个破损的数据。这样的破损数据有用。
```
1.通过与非破损数据训练的对比，破损数据训练出来的weight噪声比较小，降噪因此的名
2.破损数据一定程度上减轻了训练数据与测试数据之间的代沟，由于数据的部分被丢弃了，因而破损数据一定程度上比较接近测试数。
````

## 最小熵

“知识”有一个固有信息熵，代表它的本质信息量。但在我们彻底理解它之前，总会有未知的因素，这使得我们在表达它的时候带有冗余，所以按照我们当前的理解去估算信息熵，得到的事实上是固有信息熵的上界，而信息熵最小化意味着我们要想办法降低这个上界，也就意味着减少了未知，逼近固有信息熵。

# [Pseudo-Label](https://www.yuque.com/xf015y/nszw79/ybnrc4)

主要的思想：联合训练有标签数据和无标签数据，对没有标签的数据，选取每次权重更新时预测概率最大的类别作为其伪标签，并将其当成真实标签一样进行使用。

总体的存世函数为：
$$
L=\frac{1}{n} \sum_{m=1}^{n} \sum_{i=1}^{C} L\left(y_{i}^{m}, f_{i}^{m}\right)+\alpha(t) \frac{1}{n^{\prime}} \sum_{m=1}^{n^{\prime}} \sum_{i=1}^{C} L\left(y_{i}^{m}, f_{i}^{\prime m}\right)
$$
n表示SGD中有标签数据的mini-batch大小
$n'$表示SGD中无标签数据的mini-batch大小
$f_i^m$表示有标签数据中m个样本的输出集合
$y_i^m$表示有标签数据中m个样本的真实标签
$f_i^{\prime m}$是无标签数据中m个样本的输出集合
$y_i^{\prime m}$是无标签数据中m个样本的伪标签
$\alpha (t)$是平衡他们的系数

# 参考
1. [科学空间最小熵](https://spaces.ac.cn/archives/5448)