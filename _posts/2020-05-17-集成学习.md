---
layout:     post
title:      "集成学习"
subtitle:   " \"集思广益\""
date:       2020-05-17 15:30:00 
mathjax: true
author:     "zwt"
header-img: "img/post-bg-2015.jpg"
catalog: false
tags:
    - 机器学习
---
* TOC
{:toc}
# 1、基本概念
1. 集成学习是一种技术框架，其按照不同的思路来组合基础模型，从而达到其利断金的目的。
2. 2种常见的集成学习框架：bagging，boosting（周志华机器学习）
	- bagging：相互之间不存在强依赖关系，可以同时生成的并行化方法。
	- boosting：存在强依赖，必须串行生成的方法。

# 2、学习器的选择

1. 偏差描述预测与真实值的差异，方差描述预测值的离散程度

![](https://zwt0204.github.io//img/偏差方差.jpg)
2. 方差：就相当于模型在真实值的周边，但是比较分散
3. 偏差：模型聚集在一起，不过偏离真实值

# 3、随机森林
1. 概念：用随机的方式建立一个森林，森林里面有很多决策树，随机森林的每一个决策树之间是没有关联的。
2. 对于使用：将一个新的输入送入模型，森林中的每一个决策树都对其进行判断，采取多数表决。
3. 


# 4、AdaBoost

通过改变训练数据的分布生成不同的分类器，对多个分类器进行线性组合来得到最终的结果。

## 4.1、算法流程
1. 输入：训练数据$$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$$其中$$x_{i} \in \mathcal{X} \subseteq \mathbf{R}^{n} $$,$$y_i \in y=\{-1,+1\}$$
2. 输出：最终分类器$$G(x)$$
	- 初始化训练数据的权值分布：$$D_{1}=\left(w_{11}, \cdots, w_{1 i}, \cdots, w_{1 N}\right), \quad w_{1 i}=\frac{1}{N}, \quad i=1,2, \cdots, N$$
	- 对$$\quad m=1,2, \cdots, M$$
		- 使用具有权值分布$$D_m$$的训练数据集学习，得到基本分类器$$G_m(x):\mathcal{X} -> \{-1,+1\}$$
		- 计算$$G_m(x)$$在训练数据集上的误差率：$$e_{m}=\sum_{i=1}^{N} P\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)=\sum_{i=1}^{N} w_{m i} I\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)$$
		- 计算$$G_m(x)$$的系数：$$\alpha_m = \frac{1}{2}ln \frac{1-e_m}{e_m}$$
		- 更新训练数据权重分布：
		$$
		\begin{array}{c}
D_{m+1}=\left(w_{m+1,1}, \cdots, w_{m+1, i}, \cdots, w_{m+1, N}\right) \\
w_{m+1, i}=\frac{w_{m i}}{Z_{m}} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right), \quad i=1,2, \cdots, N
\end{array}
\\
Z_{m}=\sum_{i=1}^{N} w_{m i} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right)
		$$
	- 构建基本分类器的线性组合：$$f(x) = \sum_{m=1}^M \alpha_m G_m(x)$$，最终分类器为:$$G(x) = sign(f(x)) = sign(\sum_{m=1}^M \alpha_m G_m(x))$$

## 4.2、Adaboost前向分布算法

1. AdaBoost是加法模型，损失函数是指数函数，学习算法为前向分布算法的分类问题

2. 加法模型表示我们最终学习到的强分类器是若干弱分类器加权平均得到$$f(x) = \sum_{m=1}^M \alpha_m G_m(x)$$

3. 损失函数是指数函数:$$L(y, f(x)) = exp(-yf(x))$$

4. 前向分步算法：

   - 在第m轮迭代得到$$\alpha_m, G_m(x), f_m(x)$:$f_m(x)=f_{m-1}(x)+\alpha_m G_m(x)$$

   - 目标函数$$\arg \min _{\alpha, G} \sum_{i=1}^{N} \exp \left(-y_{i}\left(f_{m-1}\left(x_{i}\right)+\alpha G\left(x_{i}\right)\right)\right)$$

   - 令$$\bar{w}_{m i}=\exp \left(-y_{i} f_{m-1}\left(x_{i}\right)\right)$$则目标函数为：$$\arg \min _{\alpha, G} \sum_{i=1}^{N} \bar{w}_{m i} \exp \left(-y_{i} \alpha G\left(x_{i}\right)\right)$$

   - 求$$G_m^*$$:

     - 具有损失最小的第m个分类器为：$$G^{*}_{m}(x)=\arg \min _{G} \sum_{i=1}^{N} \bar{w}_{m i} I\left(y_{i} \neq G\left(x_{i}\right)\right)$$

     - 代入损失函数得到$$\alpha_{m}=\sum_{i=1}^{N} \bar{w}_{m i} \exp \left(-y_{i} \alpha_{m} G_{m}^{*}\left(x_{i}\right)\right)$$
       $$
       \begin{array}{l}
       \quad \sum_{i=1}^{N} \bar{w}_{m i} \exp \left(-y_{i} \alpha_{m} G^{*}_{m}\left(x_{i}\right)\right) \\
       =\sum_{y_{i}=G_{m}\left(x_{i}\right)} w_{m i}^{-} e^{-\alpha}+\sum_{y_{i} \neq G_{m}\left(x_{i}\right)} w_{m i}^{-} e^{\alpha} \\
       =e^{-\alpha} \sum_{y_{i}=G_{m}\left(x_{i}\right)} \bar{w}_{m i}+e^{\alpha} \sum_{y_{i} \neq G_{m}\left(x_{i}\right)} \bar{w}_{m i} \\
       =e^{-\alpha}\left(\sum_{i=1}^{N} \bar{w}_{m i}-\sum_{y_{i} \neq G_{m}\left(x_{i}\right)} \bar{w}_{m i}\right)+e^{\alpha} \sum_{y_{i} \neq G_{m}\left(x_{i}\right)} \bar{w}_{m i} \\
       =\left(e^{\alpha}-e^{-\alpha}\right) \sum_{y_{i} \neq G_{m}\left(x_{i}\right)} \bar{w}_{m i}+e^{-\alpha} \sum_{i=1}^{N} \bar{w}_{m i} \\
       =\left(e^{\alpha}-e^{-\alpha}\right) \sum_{i=1}^{N} \bar{w}_{m i} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)+e^{-\alpha} \sum_{i=1}^{N} \bar{w}_{m i}
       \end{array}
       \\
       令x = \sum_{i=1}^{N} \bar{w}_{m i} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right), y = \sum_{i=1}^{N} \bar{w}_{m i}
       $$
       
      - 对$$\alpha_m = (e^{\alpha} + e^{-\alpha})x + e^{-\alpha}y$$对$$\alpha$$求导的：$$\alpha = \frac{1}{2}ln(\frac{y}{x}-1)$$
     
      - 又因为：$$e_{m}=\frac{\sum_{i=1}^{N} \bar{w}_{m i} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)}{\sum_{i=1}^{N} \bar{w}_{m i}}$$
     
      - 所以：$$\alpha^{*}_{m}=\frac{1}{2} \ln \frac{1-e_{m}}{e_{m}}$$

## 4.3、总结
1. Adaboost只适用于二分类，扩展到多分类SAMME，SAMME.R等。
2. 可以在更新中加入学习率作为正则项。
3. 对自定义损失函数不友好。
4. 对异常点敏感。
# 5、BT

# 6、GDBT
$$
L(y, f(x)) = L(y, y^{t-1}+f_t(x))\\
将其在L(y, y^{t-1})领域做二阶泰勒级数展开有\\
L(y, y^{t-1}+f_t(x)) = L(y, y^{t-1}) + gf_t(x) + \frac{1}{2}hf^2_t(x)\\
g = \frac{\partial L(y, y^{t-1})}{\partial y^{t-1}}\\
h = \frac{\partial L(y, y^{t-1})^2}{\partial y^{t-1}}\\
求极值:g + hf_t(x)=0\\
f_t(x) = \frac{-g}{h}
$$
1. 对于平方损失，h=1.$$g = -f_t(x) = y^{t-1} - y$$,残差等于负梯度
2. 对于其他损失函数，设h=1，舍弃高阶项，相当于拟合残差的近似值。

# 7、XGBOOST
XGBoost是boosting算法的其中一种。Boosting算法的思想是将许多弱分类器集成在一起形成一个强分类器。因为XGBoost是一种提升树模型，所以它是将许多树模型集成在一起，形成一个很强的分类器。而所用到的树模型则是CART回归树模型。
## CART回归树
CART（Classification And Regression Tree）回归树是假设树为二叉树，通过不断将特征进行分裂。比如当前树结点是基于第j个特征值进行分裂的，设该特征值小于s的样本划分为左子树，大于s的样本划分为右子树。

$$
R_{1}(j, s) = {x|x^{(j)} <= s}  \\
R_{2}(j,s) = {x|x^{(j)} > s}
$$
CART回归树实质上就是在该特征维度对样本空间进行划分，CART回归树产生的目标函数为：

$$
\sum_{x_{j} \in\ R_{m}}(y_{i}-f(x_{i}))^2
$$

因此，当我们为了求解最优的切分特征j和最优的切分点s，就转化为求解这么一个目标函数：

$$
\min_{j,s} [\min_{c_{1}}\sum_{x_{j} \in\ R_{1}(j,s)}(y_i-c_1)^2 + \min_{c_{2}}\sum_{x_{j} \in\ R_{2}(j,s)}(y_i-c_2)^2 ]
$$

所以我们只要遍历所有特征的所有切分点，就能找到最优的切分特征和切分点。最终得到一颗回归树。
## 基本思想
不断的添加树
## 基本原理

### 选择什么特征进行分裂
1. 通过特征并行的方式并行计算选择要分裂的特征，尝试将各个特征作为分裂的特征，找到各个特征的最优分割点，计算根据其分割后产生的增益，选择增益最大的特征作为分裂的特征。
### 选择什么分裂点位
1. 全局扫描：将所有样本特征按照取值从小到大排列，将所有可能的分裂位置都尝试一遍，找到增益最大的分裂点，计算复杂度和叶子节点上的样本特征的取值个数成正比
2. 候选分位点：选择常数个候选分裂位置，然后从候选分裂位置中找出最优的那个。

# 8、LightGBM

# 参考
1. [知乎](https://www.zhihu.com/question/29036379/answer/43002915)
2. [CART回归树例子](https://www.cnblogs.com/limingqi/p/12421960.html)
3. [xgboost知乎](https://zhuanlan.zhihu.com/p/86816771)
4. [xgboost知乎](https://zhuanlan.zhihu.com/p/92837676)