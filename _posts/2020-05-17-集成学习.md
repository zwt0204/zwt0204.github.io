---
layout:     post
title:      "集成学习"
subtitle:   " \"集思广益\""
date:       2020-05-17 15:30:00 
author:     "zwt"
header-img: "img/post-bg-2015.jpg"
catalog: false
tags:
    - 机器学习
---
* TOC
{:toc}
# 1、基本概念
1. 集成学习是一种技术框架，其按照不同的思路来组合基础模型，从而达到其利断金的目的。
2. 三种常见的集成学习框架：[bagging，boosting和stacking](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf)
	- bagging：对训练数据有放回随机抽取，分别抽取很多个训练数据集，每个数据集训练一个模型，最后综合所有模型的结果考虑。
	- boosting：一个模型接一个模型的训练，后面的模型预测的是前一个模型的残差，最后，各个模型进行组合来得到结果。
	- stacking：当前模型训练的结果作为下次模型训练的输入，依次类推。

# 2、偏差和方差

偏差描述预测与真实值的差异，方差描述预测值的离散程度

![](https://zwt0204.github.io//img/偏差方差.jpg)

1. 模型中：
   - 偏差：模型的准确度
   - 方差：不同的训练数据集训练出的模型的测评
2. bagging中：；理论上每个学习器是独立的
	- 偏差：$E\left[\frac{\sum X_{i}}{n}\right]=E\left[X_{i}\right]$
	- 方差：$var\left(\frac{\sum X_{i}}{n}\right)=\frac{var\left(X_i\right)}{n}$
	- 所以整体的偏差是不变的，方差在逐步的减小
3. boosting中：弱模型的集成
	- 偏差：大->小
	- 方差：小
4. stacking：
	- 偏差：小
	- 方差：小
# 3、随机森林

# 4、AdaBoost

# 5、BT

# 6、DBDT

# 7、XGBOOST

# 8、LightGBM



# 参考

1. [知乎](https://www.zhihu.com/question/29036379/answer/43002915)