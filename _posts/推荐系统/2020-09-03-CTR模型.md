---
layout:     post
title:      "CTR"
subtitle:   " \"推荐\""
date:       2020-09-03 18:00:00
mathjax: true
author:     "zwt"
header-img: "img/post-bg-2015.jpg"
catalog: false
tags:
    - 推荐系统
---
* TOC
{:toc}

# 前深度学习

虽然深度学习现在遍地开花，但是传统的LR、FM等方法由于其可解释性墙，模型轻量，便于在线学习，仍然拥有很多的适用场景。同时每个模型也各有自己适用的场景， 没有一个通用模型来统一各个CTR场景。

![](https://zwt0204.github.io//img/CTR.png)

如上图所示为传统的CTR模型基本的演进过程。通过上图可以发现，整个演进以LR为基础向四周延展。

```
向下为了解决特征交叉的问题，演化为PLOY2、FM、FFM模型
向右为了使用模型，自动化的手段解决之前特征工程的难题，演化出GBDT+LR
向左为了解决时效性问题，提出FTRL
向上基于样本分组的思路增强模型的非线性，LS-PLM（MLR）模型
```
## Logistic Regression(LR)
逻辑回归是对所有特征的一个线性加权组合，然后再加上sigmod逻辑函数：
$$
\hat{y}(x)=w_{0}+w_{1} x_{1}+\ldots+w_{n} x_{n}=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}
$$

[详细查看点击](https://zwt0204.github.io/2020/09/02/lr+gdbt/)

## Degree-2 Polynomial Margin(POLY2)

POLY2提出是特征交叉的开始。由于LR的变现能力比较初级，仅仅使用单一的特征，没有办法很好的利用高纬信息，在[辛普森悖论](https://baike.baidu.com/item/%E8%BE%9B%E6%99%AE%E6%A3%AE%E6%82%96%E8%AE%BA/4475862?fr=aladdin)现象下，只是用单一 特征进行判断， 甚至会出现错误的分析。

POLY2在LR的基础上进行了暴力的特征组合,就是所有特征两两相交，因此原始的LR就变成：
$$
\hat{y}(x)=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n-1} \sum_{j=i+1}^{n} w_{i j} x_{i} x_{j}
$$

由公式了解到，POLY2是将所有的模型两两相交，暴力的组合了特征。结果是增加了特征维度，考虑了特征之间的关系，但是同时也增加了计算量，复杂度从n到$n^2$。维度很高，数据很稀疏的情况下，模型很难收敛。

## Factorization Machine(FM)

FM的优势在于为每个特征学习一个隐权重向量，在进行特征交叉计算的时候，基于隐权重向量来计算。FM在POLY2的基础上融入了矩阵分解思想，在对二阶交叉特征的系数以矩阵分解的方式调整，让系数不再是独立无关的，同时也就解决了数据稀疏导致的无法训练参数的问题。
$$
\hat{y}(\mathbf{x}):=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n} \sum_{j=i+1}^{n}\left\langle\mathbf{v}_{i}, \mathbf{v}_{j}\right\rangle x_{i} x_{j}
\\
其中,w_0 \in R, w \in R^n , V \in R^{n*k}, V 是一个n*k的向量矩阵，n是特征x的个数，k是待定参数。
\\
<v_i, v_j>表示点乘，也就是向量对应位置乘积的和。
\\
\left\langle\mathbf{v}_{i}, \mathbf{v}_{j}\right\rangle:=\sum_{f=1}^{k} v_{i, f} \cdot v_{j, f}
\\
由矩阵分解可知，对任意的正定矩阵W，都可以找到一个矩阵V，在V的维度k足够大的时候使得W= \\VV^t成立。因此，通过矩阵分解用两个向量v_i, v_j的内积近似还原表示W。同时，在拆分后，\\参数更新时是对两个向量分别更新，则在更新时，对于向量v_i，我们不需要寻找一组x_i,x_j同\\时不为0，只需要在x_i != 0 的情况下，找到任意一个样本x_k !=0就可以通过x_ix_k来更新v_i
$$

举例：
```
在商品推荐的场景下，样本有两个特征，分别是类品和品牌。某个训练样本的特征组合是（足球，阿迪达斯）。在POLY2模型中，只有当“足球”和“阿迪达斯”同时出现在一个训练样本中时，模型才能够学到这个组合特征对应的权重。而在因子分解机FM中，“足球”的的隐向量也可以根据（足球，耐克）进行更新。“阿迪达斯”的隐向量也可以根据（篮球，阿迪达斯）更新，由此一来，就大幅度的降低了模型对稀疏性的要求。
```

复杂度的降低：
$$
\sum_{i=1}^{n} \sum_{j=i+1}^{n}\left\langle\mathbf{v}_{i}, \mathbf{v}_{j}\right\rangle x_{i} x_{j}=\frac{1}{2}\left(\sum_{i=1}^{n} \sum_{j=1}^{n}\left\langle\mathbf{v}_{i}, \mathbf{v}_{j}\right\rangle x_{i} x_{j}-\sum_{i=1}^{n}\left\langle\mathbf{v}_{i}, \mathbf{v}_{i}\right\rangle x_{i} x_{i}\right)
$$
由于$$\left\langle\mathbf{v}_{i}, \mathbf{v}_{i}\right\rangle=\sum_{f=1}^{k} v_{i, f}, v_{j, f}$$所以：
$$
\begin{array}{l}
=\frac{1}{2}\left(\sum_{i=1}^{n} \sum_{j=1}^{n} \sum_{f=1}^{k} v_{i, f} v_{j, f} x_{i} x_{j}-\sum_{i=1}^{n} \sum_{f=1}^{k} v_{i, f} v_{i, f} x_{i} x_{i}\right) \\
=\frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)\left(\sum_{j=1}^{n} v_{j, f} x_{j}\right)-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right) \\
=\frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)^{2}-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right)
\end{array}
$$
因此，化简后的公式为：
$$
\hat{y}(x):=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)^{2}-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right)
$$
其计算复杂度为O(kn)

[因子机的求解详细请点击](https://zwt0204.github.io/2020/09/04/FM+FFM/)

## Field-aware Factorization Machine(FFM)

FFM是在FM的基础上增加了field的概念。

# 参考

1. [POLY2](https://zhuanlan.zhihu.com/p/153500425)
2. [矩阵分解](https://zhuanlan.zhihu.com/p/145120275)
3. [FM](https://zhuanlan.zhihu.com/p/153500425)