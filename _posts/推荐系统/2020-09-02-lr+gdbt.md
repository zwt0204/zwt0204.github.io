---
layout:     post
title:      "lr+gdbt"
subtitle:   " \"推荐\""
date:       2020-09-02 18:00:00
mathjax: true
author:     "zwt"
header-img: "img/post-bg-2015.jpg"
catalog: false
tags:
    - 推荐系统
---
* TOC
{:toc}

# LR

逻辑回归作为一种广义线性模型，假设因变量是服从伯努利分布的，在点击率预估或者推荐系统中，点击、购买时间的发生就是模型的因变量y，而用户是否点击或者购买就是一个经典的二分类问题，因此因变量就是服从伯努利分布的，所以采用LR作为CTR或者推荐的模型符合点击购买这一物理意义。

公式化的解释，LR就是讲特征进行加权求和，然后再经过sigmod将结果映射到0-1之间。具有可解释性，可以轻易的看出哪一些特征比较重要，出问题，也可以具体分析。

由于LR易于并行化，同时模型比较简单且训练开销小。使得其可以作为一个很好的baseline

但是其只是对特征进行了简单的加权，没有很好的考虑特征之间的关系，并且特征工程也很依赖于人力。所以在深度学习流行之前，facebook提出GDBT+LR的形式，将特征工程模型化。基于GDBT进行特征的交叉处理，生成新的离散特征向量，再将特征向量作为LR模型的输入。

# GDBT+LR

[GDBT](https://zwt0204.github.io/2020/06/20/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/)由多棵CART树组成，本质上市多棵回归树组成的森林。每一个节点进行贪心分裂，最终生成的树包含很多层，这就是一个特征组合的过程，样本会落在一个叶子节点上，将这个叶子结点记为1，剩下的记为0，得到一个向量。将得到的所有树的叶子结点向量拼接在一起所谓特征，作为LR的输入。

在这整个过程中，GDBT和LR是分别训练的，所以不牵扯LR的梯度回传到GDBT这个问题。
![](https://zwt0204.github.io//img/gdbt+lr.jpg)

# 

# 参考
1. [lr+gdbt]((https://zhuanlan.zhihu.com/p/76794626)
2. [代码](https://github.com/zwt0204/recommend-system)