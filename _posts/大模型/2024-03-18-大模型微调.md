---
layout:     post
title:      "微调"
subtitle:   " \"微调\""
date:       2024-03-18 18:00:00
mathjax: true
author:     "zwt"
header-img: "img/post-bg-2015.jpg"
catalog: false
tags:
    - llms
---
* TOC
{:toc}
# 为什么需要高效微调
自然语言处理的任务中，从bert出现之后，基本的流程就是预训练+微调。微调需要针对每个下游任务来进行，同时微调后的模型和原始模型是一样大的。这种形式对于bert、gpt来说还勉强可以接受，但是对于再大的模型如gpt3(175B)等就是一个很大的挑战了。
所以我们需要一些可以快速的微调类似gpt3类的大模型，同时也不需要太多的存储。
# prompt tuning

# p-tuning

# p-tuningv2

# adapt tuning

# lora
![](../../img/大模型/peft/lora.png)
[lora](https://arxiv.org/pdf/2106.09685.pdf)提出的依据：模型是过参数化的，它们有更小的内在维度，模型主要依赖于这个低的内在维度（low intrinsic dimension）去做任务适配。
# 基础
思路：
1. 在原模型侧边增加旁路，做降维升维的操作来模拟内在维度。
2. 固定原模型，只训练图中的AB矩阵。
3. A初始化高斯分布，B初始化全0（保证训练开始时旁路维0）

假设需要微调一个语言模型，需要对其参数进行更新，权重表示为$W_0+\triangle W$ ,其中$W_0$表示预训练语言的参数，$triangle W$表示需要更新的参数。如果是全量微调则微调的参数和语言模型参数一样。

而如果微调方式切换到lora的话，需要的参数如下：
首先预训练模型的参数为$W_0 \in R^{d*k}$  进而参数更新可以转换为:
$$
W_0 + \triangle W =  W_0 + BA,  B \in R^{d*r} , A \in R^{d*r}
$$
训练过程中只需要调节A和B的参数就可以。前向计算为:
$$
h = W_0x+ \triangle W x = W_0 x +BAx = (W_0 + AB)x
$$

==A矩阵不能全0初始化，B矩阵可全0初始化==
对于$h = W_0x + BAx$ ，假设$h^{()2}$，则：
$$
h_i^{(2)}=\sum_j z_{i, j} x_j=\sum_j\left(\sum_k B_{i, k} A_{k, j}\right) x_j
$$
## 训练策略
加入学习率调度器通常可以稳定训练，余弦退火是一种常用的学习率调度器，首先从一个较高的学习率开始平滑递减，以一种类似余弦函数的方式逼近零点，在使用的时候通常是半周期的变体，即在训练过程中只完成半个余弦周期，如下图所示：
![](../../img/大模型/peft/余弦退火.png)
经验：对于SGD提升比较明显，但是对Adam，AdamW影响较小。

==？为什么不使用SGD作为优化函数，其相对Adam等优化器节省了多余参数的存储，可以降低对GPU显存的使用==

对数据循环多次使用，并不会提升性能，相反可能会导致结果的恶化。

lora在前向传播的时候引入了扩展系数，用于将lora的权重应用于预训练权重，涉及到的参数有r，alpha。具体计算为$scaling = alpha / r$，$weight += (lora_b @ lora_a) * scaling$。

通常设置r 为alpha的一半性能较好，但是对一些特定数据集也可能有更好的经验参数。

学习率设置3e-4，梯度累积的使用，weight_decay=0.1,warmup_steps=100
如果是SGD的话，学习率0.1。动量0.9
# qlora
![](../../img/大模型/peft/qlora.png)
[论文](https://arxiv.org/abs/2305.14314)
创新点：
```
1. 一种新的数据类型4位NormalFloat
2. 双重量化以减少平均内存占用
3. 分页优化器来管理内存高峰
````

计算流程：
1. 一个基本模型权重的存储数据类型NF4。
2. 一个用语执行计算的数据类型BF16。
3. 权重从存储的数据类型反量化位计算数据类型以执行前向后向传递。
4. 传递过程中仅计算使用BF16的LoRA参数的权重梯度。
5. 权重在需要时解压缩，所以在训练和推理期间内存使用量保持较低。
## Quantize
模型的大小通常由其参数的数量及其精度决定，常见的精度有全精度float32(FP32)、半精度float16(FP16)和bfloat16(BF16)。

FP32：单精度浮点数，用8bit 表示指数，23bit 表示小数。使用此数据类型，可以表示各种浮点数，并且支持大多硬件。
FP16：半精度浮点数，用5bit 表示指数，10bit 表示小数。FP16 表示整数范围较小，但是尾数精度较高。
BF16：是对FP32单精度浮点数截断数据，用8bit 表示指数，7bit 表示小数。BF16 可表示的整数范围与FP32一样广泛。但只有新的硬件(A100\3090\4090等)才支持，V100/昇腾910等不支持

![](../../img/大模型/peft/量化.jpeg)
量化的本质实际是从一种数据类型舍入到另一种数据类型，通常包含量化和反量化两步：
假如我们有两组数据类型A、B，A可以表示的数值为[0, 1, 2, 3, 4, 5]，B可以表示的数值为[0, 2, 4]。我们要做的便是：
```
将数据范围从A标准化为B。数据类型A表示的向量为[3, 1, 2, 3]。
找到向量[3, 1, 2, 3]的最大绝对值3
向量[3, 1, 2, 3]除以最大值3：[3, 1, 2, 3]->[1, 0.33, 0.66, 1.0]
将向量[1, 0.33, 0.66, 1.0]与B的数据范围4相乘：[1, 0.33, 0.66, 1.0]->[4.0, 1.33, 2.66, 4.0]
将向量[4.0, 1.33, 2.66, 4.0]中的每个值用B中最接近的数值表示：[4.0, 1.33, 2.66, 4.0] -> [4, 2, 2, 4]。
用B中最接近的数值表示A 。
[4, 2, 2, 4]除以4->[1.0, 0.5, 0.5, 1.0]
乘以量化过程中找到的最大的绝对值：[1.0, 0.5, 0.5, 1.0] -> [3.0, 1.5, 1.5, 3.0]
近似表示：[3.0, 1.5, 1.5, 3.0] -> [3, 2, 2, 3]
```
==注意经过量化和反量化之后会存在一定的精度误差，解决方法是使用更多的量化参数：比如不同的区间使用不同的独立的量化参数==

借鉴[LLM.int8](https://arxiv.org/pdf/2208.07339.pdf)中的vector-wise想量化+混合精度分解。
![](../../img/大模型/peft/block-wise.jpeg)
如上图所示：先找到离群点，离群点通过fp16计算，其他的通过int8量化计算，最终再反量化回去和离群点相加处理。
## 4-bit NormalFloat
是一种建立在分位数量化技术的基础之上的一种信息理论上最优的数据类型。
对于预训练的神经网络的权重来说通常具有标准差位0的正态分布性质，所以我们可以通过缩放系数来将所有的权值改为固定期望值，从而使得该分布适合我们的数据类型范围，一旦权重范围和数据类型范围匹配，我们就可以进行了量化了。

分位数量化技术的主要思想便是将数值尽量落到均值为0，标准差为[-1,1]的正态分布的固定期望值上。离群值对于模型量化的影响极其重要，而由于分位数估计算法的近似性质，精度量化对于离群值又有很大的误差。分位数量化技术使得每个量化分区中具有相等的期望值，相等的期望值可以避免昂贵的分位数估计和近似误差，使得精确的分位数估计在计算上可行。

分位数量化步骤：
1. 估计N(0,1)分布的$2^{k+1}$个分位数，得到正太分布的k-bit位量化数据类型。
2. 将其值归一化到[-1,1]范围内。
3. 通过absmax来重新缩放权重张量的标准差，以获得k-bit的数据形式。
分位数$q_i$的计算过程：
$$
q_i = 1/2((Q_x(i/(z^k +1))) + Q_X((1+i)/(2^k + 1)))
$$
它在量化过程中保留了零点，并使用所有$2^k$位来表示k-bit数据类型。这种数据类型通过估计两个范围的分位数$q_i$来创建一个非对称的数据类型，这两个范围分别是负数部分[-1,0]的$r << min(d, k)$和正数部分[0,1]的$2^{k-1}+1$ 。然后，它统一了这两组分位数$q_i$，并从两组中都出现的两个零中移除一个。这种结果数据类型在每个量化bin中都有相等的期望值数量，因此被称为$2^{k-1}+1$。

以NF4，即k=4为例，标准正态分布量化函数把[-1, 0]分成7份，然后生成[-1, ..., 0]共8个分位数, 把[0, 1]分成8份，然后生成[0, ..., 1]共9个分位数，两个合起来去掉一个0就生成全部的16个分位数了。
```
我们的目标是找到等面积的量化区间，使得量化区间左右两侧的面积相等。这意味着我们不从正态分布的0和1量化区间开始，而是从一个偏移量量化区间开始。代码片段中称之为"offset"，其值为1-1/(215)。如果我们有一个非对称的数据类型，其中一侧的间隔等于每个量化区间周围的16个“半个”，而另一侧只有15个“半个”。因此，平均偏移量为(1-1/(2*15) + 1-1/(2*16))/2 = 0.9677083。
```
## Double Quantization
Double Quantization是将额外的量化常数进行二次量化以减小内存开销的过程。例如每64个参数块共享一个32bit的量化常数， 这样的话相当于每一个参数的量化额外开销为32/64 = 0.5 bit。这个总体来说也是比较大的一个开销，所以为了进一步优化这个量化开销，我们对其进行二次量化(Double Quantization)，即把第一次32bit量化的输出作为第二次量化的输入，我们采用256的块大小对量化常数进行FP8量化，这样的话，我们可以把每个参数的量化额外开销降低到：8/16 + 32/(64*256) = 0.127bit.

## Paged Optimizers
使用NVIDIA统一内存功能，该功能在CPU和GPU之间进行自动page对page传输，以便在GPU偶尔OOM的情况仍然下进行模型训练和微调。 可以理解成显存偶发OOM时，QLoRA会将优化器状态自动的驱逐到CPU RAM，当在优化器更新步骤中需要内存时，它们会被分页回GPU内存，从而保证训练正常训练下去。

# 参考

1. [知乎1](https://www.zhihu.com/tardis/zm/art/623543497?source_id=1003)
2. [智源1](https://hub.baai.ac.cn/view/33321)
3. [qlora](https://zhuanlan.zhihu.com/p/634516004?utm_id=0)