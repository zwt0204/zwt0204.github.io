---
layout:     post
title:      "微调"
subtitle:   " \"微调\""
date:       2024-03-18 18:00:00
mathjax: true
author:     "zwt"
header-img: "img/post-bg-2015.jpg"
catalog: false
tags:
    - llms
---
* TOC
{:toc}
# 为什么需要高效微调
自然语言处理的任务中，从bert出现之后，基本的流程就是预训练+微调。微调需要针对每个下游任务来进行，同时微调后的模型和原始模型是一样大的。这种形式对于bert、gpt来说还勉强可以接受，但是对于再大的模型如gpt3(175B)等就是一个很大的挑战了。
所以我们需要一些可以快速的微调类似gpt3类的大模型，同时也不需要太多的存储。
# prompt tuning

# p-tuning

# p-tuningv2

# adapt tuning

# lora
![](../../img/大模型/peft/lora.png)
[lora](https://arxiv.org/pdf/2106.09685.pdf)提出的依据：模型是过参数化的，它们有更小的内在维度，模型主要依赖于这个低的内在维度（low intrinsic dimension）去做任务适配。

思路：
1. 在原模型侧边增加旁路，做降维升维的操作来模拟内在维度。
2. 固定原模型，只训练图中的AB矩阵。
3. A初始化高斯分布，B初始化全0（保证训练开始时旁路维0）

假设需要微调一个语言模型，需要对其参数进行更新，权重表示为$W_0+\triangle W$ ,其中$W_0$表示预训练语言的参数，$triangle W$表示需要更新的参数。如果是全量微调则微调的参数和语言模型参数一样。

而如果微调方式切换到lora的话，需要的参数如下：
首先预训练模型的参数为$W_0 \in R^{d*k}$  进而参数更新可以转换为:
$$
W_0 + \triangle W =  W_0 + BA,  B \in R^{d*r} , A \in R^{d*r}
$$
训练过程中只需要调节A和B的参数就可以。前向计算为:
$$
h = W_0x+ \triangle W x = W_0 x +BAx = (W_0 + AB)x
$$

==A矩阵不能全0初始化，B矩阵可全0初始化==
对于$h = W_0x + BAx$ ，假设$h^{()2}$，则：
$$
h_i^{(2)}=\sum_j z_{i, j} x_j=\sum_j\left(\sum_k B_{i, k} A_{k, j}\right) x_j
$$


# qlora


# 参考
1. [知乎1](https://www.zhihu.com/tardis/zm/art/623543497?source_id=1003)