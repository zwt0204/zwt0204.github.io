---
layout:     post
title:      "Tinybert"
subtitle:   " \"bert家族\""
date:       2020-07-06 15:30:00 
mathjax: true
author:     "zwt"
header-img: "img/post-bg-2015.jpg"
catalog: false
tags:
    - bert
---
* TOC
{:toc}

# 基础
缩小模型的方法：
```
Distillation:蒸馏，将大模型（可以是集成模型）的精华注入到小模型，使其具备接近大模型的能力。
Quantizationb：量化，将高精度模型用低精度表示。
Pruning：剪枝，将模型中作用小的部分舍弃。
```
==注意：对于nlp来说，蒸馏时目前比较实用的方法==

概念解释：
```
teacher：原始模型
student：新模型
transfer set：用于迁移teacher知识，训练studebt的数据集合
soft target：teacher输出的预测结果（通常是softmax之后的概率）
hard target：样本原本的标签
temperature：蒸馏目标函数中的超参数
born-again network：蒸馏的一种，指的是student和teacher的结构和尺寸完全一样
teacher annealing：防止student的标签杯teacher限制，在蒸馏的时候逐渐减少soft targets的权重。
```

# 比较直接的方法
==好模型的目标不是拟合训练数据，而是学习如何泛化到新的数据==
以复杂模型得到的结果作为软目标，来进行模型的蒸馏。
$$
q_{i}=\frac{\exp \left(z_{i} / T\right)}{\sum_{j} \exp \left(z_{j} / T\right)}
$$
```
比如模型训练的过程中，模型会对每一种分类输出概率，正确的类别概率可能很大，其他的类型概率很小，但是这个很小的概率也表示了类别之间的一些相关特征。通过将这个softmax的结果进行平滑，也就是使得结果的熵变大，那么相对于真实的输出，软目标可以携带更多的信息。上面公式的T就表示对softmax结果的平滑处理。

将真实的标签和软标签同时使用，将两个目标函数加权平均，效果更佳。
```

理解：
```
以大模型的预测结果为最后的标签，基于小模型去以这个预测标签为label进行有监督的训练，从而期望可以学习到大模型中的有用信息。
```



# 参考
1. [BERT 瘦身之路：Distillation，Quantization，Pruning](https://zhuanlan.zhihu.com/p/86900556)