---
layout:     post
title:      "Tinybert"
subtitle:   " \"bert家族\""
date:       2020-07-06 15:30:00 
mathjax: true
author:     "zwt"
header-img: "img/post-bg-2015.jpg"
catalog: false
tags:
    - bert
---
* TOC
{:toc}

# 基础
缩小模型的方法：
```
Distillation:蒸馏，将大模型（可以是集成模型）的精华注入到小模型，使其具备接近大模型的能力。
Quantizationb：量化，将高精度模型用低精度表示。
Pruning：剪枝，将模型中作用小的部分舍弃。
```
==注意：对于nlp来说，蒸馏时目前比较实用的方法==

概念解释：
```
teacher：原始模型
student：新模型
transfer set：用于迁移teacher知识，训练studebt的数据集合
soft target：teacher输出的预测结果（通常是softmax之后的概率）
hard target：样本原本的标签
temperature：蒸馏目标函数中的超参数
born-again network：蒸馏的一种，指的是student和teacher的结构和尺寸完全一样
teacher annealing：防止student的标签杯teacher限制，在蒸馏的时候逐渐减少soft targets的权重。
```

# 比较直接的方法
==好模型的目标不是拟合训练数据，而是学习如何泛化到新的数据==
以复杂模型得到的结果作为软目标，来进行模型的蒸馏。
$$
q_{i}=\frac{\exp \left(z_{i} / T\right)}{\sum_{j} \exp \left(z_{j} / T\right)}
$$
```
比如模型训练的过程中，模型会对每一种分类输出概率，正确的类别概率可能很大，其他的类型概率很小，但是这个很小的概率也表示了类别之间的一些相关特征。通过将这个softmax的结果进行平滑，也就是使得结果的熵变大，那么相对于真实的输出，软目标可以携带更多的信息。上面公式的T就表示对softmax结果的平滑处理。

将真实的标签和软标签同时使用，将两个目标函数加权平均，效果更佳。
```

理解[Distilling the Knowledge in a Neural Network](https://arxiv.org/pdf/1503.02531.pdf)：
```
以大模型的预测结果为最后的标签，基于小模型去以这个预测标签为label进行有监督的训练，从而期望可以学习到大模型中的有用信息。
```

注意：
```
这种方法理论上是可以迁移任何的模型，但是对于模型的中间结果利用的不完全，也就是说只是利用了模型的输出结果的知识，模型的其他的中间知识并没有利用到。所以就更倾向于在基础结构相似的模型之间进行蒸馏，这样就可以充分的利用中间结果信息，原始model的所有的知识都可以被利用起来。DistillBert，TinyBert，就是其中的典型。
```

一些公式以及相关解释：
$$
\begin{array}{c}
L_{s o f t}=-\sum_{j}^{N} p_{j}^{T} \log \left(q_{j}^{T}\right)=-\sum_{j}^{N} \frac{\exp \left(v_{j} / T\right)}{\sum_{k}^{N} \exp \left(v_{k} / T\right)} \frac{z_{j} / T}{\sum_{k}^{N} \exp \left(z_{k} / T\right)} \\
\frac{\partial L_{s o f t}}{\partial z_{i}}=-\frac{1}{T} \frac{\exp \left(v_{i} / T\right)}{\sum_{k}^{N} \exp \left(v_{k} / T\right)} \frac{\sum_{k}^{N} \exp \left(z_{k} / T\right)-\frac{z_{j}}{T} \exp \left(z_{j} / T\right)}{\left(\sum_{k}^{N} \exp \left(z_{k} / T\right)\right)^{2}} \\
=-\frac{1}{T} \frac{\exp \left(v_{i} / T\right)}{\sum_{k}^{N} \exp \left(v_{k} / T\right)}\left(\frac{1}{\sum_{k}^{N} \exp \left(z_{k} / T\right)}-\frac{1}{T} \frac{z_{j} \exp \left(z_{j} / T\right)}{\left(\sum_{k}^{N} \exp \left(z_{k} / T\right)\right)^{2}}\right)
\end{array}\\
\begin{array}{l}
L_{h a r d}=-\sum_{j}^{N} c_{j} \log \left(q_{j}^{1}\right)=-\sum_{j}^{N} c_{j} \frac{z_{i}}{\sum_{k}^{N} \exp \left(z_{k}\right)} \\
\frac{\partial L_{h a r d}}{\partial z_{i}}=-c_{i} \frac{1}{\sum_{k}^{N} \exp \left(z_{k}\right)}
\end{array}\\
因为：\frac{1}{\sum_{k}^{N} \exp \left(z_{k}\right)} \approx \frac{T}{\sum_{k}^{N} \exp \left(z_{k} / T\right)}
$$
所以在同时使用soft target和hard target的时候，需要在soft target之前加上系数$$T^2$$，这样才能保证两个的贡献度保持一致。
# DistillBert
模型结构：
![](https://zwt0204.github.io//img/distillBert.png)

模型结构的描述：
```
每两层去掉一层（作者调研发现隐藏层的维度的变化比层数的变化对计算性能的影响较小）
去掉了token type embedding和pooler
每一层加上初始化，用teacher模型初始化student模型
```

损失函数的定义：
LCE损失:计算soft target和student的输出计算KL散度
$$
\text {softmax-temperature: } p_{i}=\frac{\exp \left(z_{i} / 1\right)}{\sum_{j} \exp \left(z_{j} / T\right)}
$$
Lcos损失：计算teacher hidden state和student hidden state的余弦相似度
Lmlm损失：原始的bert的masked language model的损失函数。


# Tinybert




# 参考
1. [BERT 瘦身之路：Distillation，Quantization，Pruning](https://zhuanlan.zhihu.com/p/86900556)
2. [BERT知识蒸馏综述](https://zhuanlan.zhihu.com/p/106810758)
3. [代码](https://github.com/zwt0204/torch_model/tree/master/Distilling)
4. [知识蒸馏(Knowledge Distillation) 经典之作](https://zhuanlan.zhihu.com/p/102038521?utm_source=wechat_timeline)
5. [语义表示模型新方向《DistillBert》](https://zhuanlan.zhihu.com/p/89522799)